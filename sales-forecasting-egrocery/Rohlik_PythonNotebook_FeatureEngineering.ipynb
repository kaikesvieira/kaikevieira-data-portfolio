{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sjm8lBaLOem"
   },
   "source": [
    "# **1. Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mnioeyRLOeo"
   },
   "source": [
    "## Description\n",
    "\n",
    "**Why This Matters**\n",
    "\n",
    "Accurate sales forecasts are crucial for planning process, supply chain processes, delivery logistics and inventory management. By optimizing forecasts, we can minimize waste and streamline operations, making our e-grocery services more sustainable and efficient.\n",
    "\n",
    "**Your Impact**\n",
    "\n",
    "Your participation in this challenge will directly contribute to Rohlik mission of sustainable and efficient e-grocery delivery. Your insights will help us enhance customer service and achieve a greener future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvhPlaLJLOep"
   },
   "source": [
    "We are relaunching the Challenge with prizes.\n",
    "\n",
    "Rohlik Group, a leading European e-grocery innovator, is revolutionising the food retail industry. We operate across 11 warehouses in Czech Republic, Germany, Austria, Hungary, and Romania.\n",
    "\n",
    "We are now transitioning from the Rohlik Orders Forecasting Challenge to the Rohlik Sales Forecasting Challenge, as we continue with our set of challenges. This challenge focuses on predicting the sales of each selected warehouse inventory for next 14 days using historical sales data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmwjnKCWLOer"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Submissions are evaluated on Weighted Mean Absolute Error (WMAE) between the predicted sales and the actual sales. Weights for the test evaluation can be found in the Data section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn10LuHRLOer"
   },
   "source": [
    "## Submission File\n",
    "\n",
    "For each ID in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:\n",
    "\n",
    "id,sales_hat\n",
    "\n",
    "840_2024-06-10,12.01\n",
    "\n",
    "2317_2024-06-15,13.32\n",
    "\n",
    "738_2024-06-10,14.12\n",
    "\n",
    "3894_2024-06-11,3.03\n",
    "\n",
    "3393_2024-06-08,53.03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzQAZ8W-LOer"
   },
   "source": [
    "## Prizes\n",
    "Leaderboard prizes\n",
    "\n",
    "1st place - $4,000\n",
    "\n",
    "2nd place - $4,000\n",
    "\n",
    "3rd place - $2,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IywSr2PLOes"
   },
   "source": [
    "## Citation\n",
    "\n",
    "MichalKecera. Rohlik Sales Forecasting Challenge. https://kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2, 2024. Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0opgBH9ILOes"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeIeoID4LOes"
   },
   "source": [
    "## Dataset Description\n",
    "You are provided with historical sales data for selected Rohlik inventory and date. IDs, sales, total orders and price columns are altered to keep the real values confidential. Some features are not available in test as they are not known at the moment of making the prediction. The task is to forecast the sales column for a given id, constructed from unique_id and date (e. g. id 1226_2024-06-03 from unique_id 1226 and date 2024-06-03), for the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtH0nBRILOes"
   },
   "source": [
    "## Files\n",
    "- **sales_train.csv** - training set containing the historical sales data for given date and inventory with selected features described below\n",
    "- **sales_test.csv** - full testing set\n",
    "- **inventory.csv** - additional information about inventory like its product (same products across all warehouses share same product unique id and name, but have different unique id)\n",
    "- **solution.csv** - full submission file in the correct format\n",
    "- **calendar.csv** - calendar containing data about holidays or warehouse specific events, some columns are already in the train data but there are additional rows in this file for dates where some warehouses could be closed due to public holiday or Sunday (and therefore they are not in the train set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0zt0zXcLOet"
   },
   "source": [
    "## Columns\n",
    "**sales_train.csv** and **sales_test.csv**\n",
    "\n",
    "- `unique_id` - unique id for inventory\n",
    "- `date` - date\n",
    "- `warehouse` - warehouse name\n",
    "- `total_orders` - historical orders for selected Rohlik warehouse known also for test set as a part of this challenge\n",
    "- `sales` - Target value, sales volume (either in pcs or kg) adjusted by availability. The sales with lower availability than 1 are already adjusted to full potential sales by Rohlik internal logic. There might be missing dates both in train and test for a given inventory due to various reasons. This column is missing in test.csv as it is target variable.\n",
    "- `sell_price_main` - sell price\n",
    "- `availability` - proportion of the day that the inventory was available to customers. The inventory doesn't need to be available at all times. A value of 1 means it was available for the entire day. This column is missing in test.csv as it is not known at the moment of making the prediction.\n",
    "- `type_0_discount`, type_1_discount, … - Rohlik is running different types of promo sale actions, these show the percentage of the original price during promo ((original price - current_price) / original_price). Multiple discounts with different type can be run at the same time, but always the highest possible discount among these types is used for sales. Negative discount value should be interpreted as no discount.\n",
    "\n",
    "**inventory.csv**\n",
    "\n",
    "- `unique_id` - inventory id for a single keeping unit\n",
    "- `product_unique_id` - product id, inventory in each warehouse has the same product unique id (same products across all warehouses has the same product id, but different unique id)\n",
    "- `name` - inventory id for a single keeping unit\n",
    "L1_category_name, L2_category_name, … - name of the internal category, the higher the number, the more granular information is present\n",
    "- `warehouse` - warehouse name\n",
    "\n",
    "**calendar.csv**\n",
    "\n",
    "- `warehouse` - warehouse name\n",
    "- `date` - date\n",
    "- `holiday_name` - name of public holiday if any\n",
    "- `holiday` - 0/1 indicating the presence of holidays\n",
    "- `shops_closed` - public holiday with most of the shops or large part of shops closed\n",
    "- `winter_school_holidays` - winter school holidays\n",
    "- `school_holidays` - school holidays\n",
    "\n",
    "**test_weights.csv**\n",
    "\n",
    "- `unique_id` - inventory id for a single keeping unit\n",
    "- `weight` - weight used for final metric computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **Import and merge DFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the correct path using forward slashes\n",
    "path = r\"C:\\\\Users\\defaultuser0\\\\OneDrive\\Documents\\\\GitHub\\\\CapstoneProject-RohlikSalesForecasting\\\\rohlik-sales-forecasting-challenge-v2\\Data\"\n",
    "\n",
    "# Load cleaned datasets\n",
    "df = pd.read_csv(fr\"{path}\\dataframes\\df_cleaned.csv\")\n",
    "sales_train = pd.read_csv(fr\"{path}\\sales_train.csv\", index_col=False)\n",
    "sales_test = pd.read_csv(fr\"{path}\\sales_test.csv\", index_col=False)\n",
    "weights = pd.read_csv(fr\"{path}\\test_weights.csv\", index_col=False)\n",
    "solution = pd.read_csv(fr\"{path}\\solution.csv\", index_col=False)\n",
    "inventory = pd.read_csv(fr\"{path}\\inventory.csv\", index_col=False)\n",
    "calendar = pd.read_csv(fr\"{path}\\calendar.csv\", index_col=False)\n",
    "frankfurt_weather = pd.read_csv(fr\"{path}\\weather\\frankfurt_weather.csv\", index_col=False)\n",
    "brno_weather = pd.read_csv(fr\"{path}\\weather\\brno_weather.csv\", index_col=False)\n",
    "budapest_weather = pd.read_csv(fr\"{path}\\weather\\budapest_weather.csv\", index_col=False)\n",
    "munich_weather = pd.read_csv(fr\"{path}\\weather\\munich_weather.csv\", index_col=False)\n",
    "prague_weather = pd.read_csv(fr\"{path}\\weather\\prague_weather.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Date Features\n",
    "\"\"\"\n",
    "# Ensure the 'date' column is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "sales_train['date'] = pd.to_datetime(sales_train['date'])\n",
    "sales_test['date'] = pd.to_datetime(sales_test['date'])\n",
    "\n",
    "# Extract date features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['month'] = df['date'].dt.month\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "df['week_of_month'] = ((df['date'].dt.day - 1) // 7) + 1\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.day_of_week\n",
    "df['day_of_year'] = df['date'].dt.day_of_year \n",
    "\n",
    "\"\"\"\n",
    "sin and cos to capture cyclical or seasonal patterns in time-related data\n",
    "\"\"\"\n",
    "\n",
    "# day\n",
    "df['dayofweek_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['dayofweek_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['dayofmonth_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "df['dayofmonth_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "df['dayofyear_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365) # Correlation 0.97 with 'month_sin'\n",
    "df['dayofyear_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365) # Correlation 0.95 with 'month_cos'\n",
    "\n",
    "# week\n",
    "df['weekofyear_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52) # Correlation 1 with 'dayofyear_sin'\n",
    "df['weekofyear_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52) # Correlation 0.96 with 'month_cos'\n",
    "df['weekofmonth_sin'] = np.sin(2 * np.pi * df['week_of_month'] / 4)\n",
    "df['weekofmonth_cos'] = np.cos(2 * np.pi * df['week_of_month'] / 4)\n",
    "\n",
    "# month\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12) # High correlation with multiple features\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# quarter\n",
    "df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "\n",
    "# drop month: High correlation with multiple features in past tests\n",
    "df.drop(columns=['quarter'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Treating `discount` columns as instructed in Overview\n",
    "\"\"\"\n",
    "# Create discount column by selecting max discount value across discount columns\n",
    "discount_cols = [col for col in df.columns if col.startswith('type_') and col.endswith('_discount')]\n",
    "df['discount'] = df[discount_cols].max(axis=1)\n",
    "\n",
    "# drop discount columns\n",
    "df.drop(columns = ['type_0_discount', 'type_1_discount', 'type_2_discount',\n",
    "                            'type_3_discount', 'type_4_discount', 'type_5_discount',\n",
    "                            'type_6_discount'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Merging Weather info\n",
    "\"\"\"\n",
    "\n",
    "# sorting the dataframe by date\n",
    "df.sort_values('date', inplace=True)\n",
    "\n",
    "# mapping cities\n",
    "df['city'] = df['warehouse'].map({'Frankfurt_1': 'Frankfurt',\n",
    "                                  'Brno_1': 'Brno',\n",
    "                                  'Budapest_1': 'Budapest',\n",
    "                                    'Munich_1': 'Munich',\n",
    "                                   'Prague_1': 'Prague',\n",
    "                                   'Prague_2': 'Prague',\n",
    "                                   'Prague_3': 'Prague'})\n",
    "\n",
    "# List of weather DataFrames and corresponding city names\n",
    "weather_dfs = [\n",
    "    (frankfurt_weather, 'Frankfurt'),\n",
    "    (brno_weather, 'Brno'),\n",
    "    (budapest_weather, 'Budapest'),\n",
    "    (munich_weather, 'Munich'),\n",
    "    (prague_weather, 'Prague')\n",
    "]\n",
    "\n",
    "for df_weather, city_name in weather_dfs:\n",
    "    df_weather.rename(columns={\"datetime\": \"date\"}, inplace=True)\n",
    "    df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "    df_weather['city'] = city_name\n",
    "    \n",
    "# Concatenate all weather data into one\n",
    "weather_df_list = [frankfurt_weather, brno_weather, budapest_weather, munich_weather, prague_weather]\n",
    "full_weather_df = pd.concat(weather_df_list, ignore_index=True)\n",
    "\n",
    "# Rename columns for consistency\n",
    "full_weather_df.rename(columns={\"name\": \"latitudelongitude\"}, inplace=True)\n",
    "\n",
    "# Final merge\n",
    "df = df.merge(full_weather_df, on=['date', 'city'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Concatenaing category columns\n",
    "\"\"\"\n",
    "df['category'] = (\n",
    "    df['L1_category_name_en'].astype(str) + \"_\" +\n",
    "    df[\"L2_category_name_en\"].str.split('_').str[-1] + \"_\" +\n",
    "    df[\"L3_category_name_en\"].str.split('_').str[-1] + \"_\" +\n",
    "    df[\"L4_category_name_en\"].str.split('_').str[-1]\n",
    ")\n",
    "\n",
    "df.drop(columns=[\"L1_category_name_en\",\n",
    "                    \"L2_category_name_en\",\n",
    "                    \"L3_category_name_en\",\n",
    "                    \"L4_category_name_en\"],\n",
    "        inplace=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Calculates days to shops_closed\n",
    "\"\"\"\n",
    "\n",
    "# Forward fill the dates where shops are closed (i.e., use the last closure date)\n",
    "df['next_closure_date'] = df['date'].where(df['shops_closed'] == 1).bfill()\n",
    "\n",
    "# Calculate the number of days after the last shop closure\n",
    "df['days_next_closure'] = (df['next_closure_date'] - df['date']).dt.days\n",
    "\n",
    "# fill NaN values with 0\n",
    "df['days_next_closure'].fillna(0, inplace=True)\n",
    "\n",
    "# drop next closure\n",
    "df.drop(columns=['next_closure_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Calculates days after shops_closed and day_after_closing\n",
    "\"\"\"\n",
    "\n",
    "# Forward fill the dates where shops are closed (i.e., use the last closure date)\n",
    "df['last_closure_date'] = df['date'].where(df['shops_closed'] == 1).ffill()\n",
    "\n",
    "# Calculate the number of days after the last shop closure\n",
    "df['days_after_closure'] = (df['date'] - df['last_closure_date']).dt.days\n",
    "\n",
    "# fill NaN values with 0\n",
    "df['days_after_closure'].fillna(0, inplace=True)\n",
    "\n",
    "# Bool value for shops_closed eve\n",
    "df['day_after_closing'] = np.where(df['days_after_closure'] == 1, 1, 0)\n",
    "\n",
    "# drop next closure\n",
    "df.drop(columns=['last_closure_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Mean availability\n",
    "\"\"\"\n",
    "\n",
    "# Filter data before the given date\n",
    "filtered_df = df[df['date'] <= sales_train['date'].max()]\n",
    "\n",
    "# Group by 'unique_id' and calculate mean availability\n",
    "mean_availability = filtered_df.groupby('unique_id')['availability'].mean().reset_index()\n",
    "\n",
    "# Rename the resulting column\n",
    "mean_availability.rename(columns={'availability': 'mean_availability'}, inplace=True)\n",
    "\n",
    "# Merge mean availability with the original DataFrame\n",
    "df = df.merge(mean_availability, on='unique_id', how='left')\n",
    "\n",
    "# Drop availability column\n",
    "df.drop(columns=['availability'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Target encoding categorical variables](https://mlbook.explained.ai/catvars.html#target-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Mean Sales per unique_id\n",
    "\"\"\"\n",
    "\n",
    "# Filter data before the given date\n",
    "filtered_df = df[df['date'] <= sales_train['date'].max()]\n",
    "\n",
    "# Group by 'unique_id' and calculate the mean sales\n",
    "mean_sales= filtered_df.groupby('unique_id')['sales'].mean().reset_index()\n",
    "\n",
    "# Rename the resulting column\n",
    "mean_sales.rename(columns={'sales': 'mean_sales'}, inplace=True)\n",
    "\n",
    "# merging\n",
    "df = df.merge(mean_sales, on='unique_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create rolling features\n",
    "# daily_sales = df_frank.groupby(\"date\")[\"mean_sales\"].sum().reset_index().rename(columns={\"mean_sales\":\"daily_mean_sales_sum\"})\n",
    "# daily_sales_mean = df_frank.groupby(\"date\")[\"mean_sales\"].mean().reset_index().rename(columns={\"mean_sales\":\"daily_mean_sales_mean\"})\n",
    "# daily_sales_std= df_frank.groupby(\"date\")[\"mean_sales\"].std().reset_index().rename(columns={\"mean_sales\":\"daily_mean_sales_std\"})\n",
    "\n",
    "# # Aggregate info\n",
    "# daily_sales = daily_sales.merge(daily_sales_mean, on='date', how='left')\n",
    "# daily_sales = daily_sales.merge(daily_sales_std, on='date', how='left')\n",
    "\n",
    "# # Step 2: Calculate rolling statistics globally\n",
    "# for i in [1, 3, 7, 15, 30, 60, 90, 120, 180, 365]:\n",
    "#     daily_sales[f\"Rolling_{i}D_Mean\"] = daily_sales[\"daily_mean_sales_sum\"].rolling(window=i, min_periods=1).mean()\n",
    "#     daily_sales[f\"Rolling_{i}D_Sum\"] = daily_sales[\"daily_mean_sales_sum\"].rolling(window=i, min_periods=1).sum()\n",
    "    \n",
    "# df_frank = df_frank.merge(daily_sales, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features for 1-day, 2-day, and 3-day previous sales\n",
    "daily_lag_sales = df.groupby(\"date\")['mean_sales'].sum().shift(1).reset_index().rename(columns={'mean_sales':'lag_1D'})\n",
    "daily_lag_sales_2D = df.groupby(\"date\")['mean_sales'].sum().shift(2).reset_index().rename(columns={'mean_sales':'lag_2D'})\n",
    "daily_lag_sales_3D = df.groupby(\"date\")['mean_sales'].sum().shift(3).reset_index().rename(columns={'mean_sales':'lag_3D'})\n",
    "\n",
    "# Aggregate results\n",
    "daily_lag_sales = daily_lag_sales.merge(daily_lag_sales_2D, on = 'date', how='left')\n",
    "daily_lag_sales = daily_lag_sales.merge(daily_lag_sales_3D, on = 'date', how='left')\n",
    "\n",
    "# Merge DFs\n",
    "df = df.merge(daily_lag_sales, on='date', how='left')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Feature Engineering Cookbook - Second Edition\n",
    "\n",
    "[Transforming variables with the logarithm function](https://learning-oreilly-com.stclair.idm.oclc.org/library/view/python-feature-engineering/9781804611302/B18894_03.xhtml#_idParaDest-90)\n",
    "\n",
    "*To access this book, make sure to Sign in the MyStClairCollege Library Search OR and O'REILLY active subscription*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nCheck and deal with skewness \\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Check and deal with skewness \n",
    "\"\"\"\n",
    "\n",
    "# bin_columns = [\"holiday\", \"winter_school_holidays\", \"school_holidays\"]\n",
    "\n",
    "\n",
    "\n",
    "# # Check skewness for all numeric columns\n",
    "# skewness = df_frank.drop(columns=bin_columns, axis = 1).skew(numeric_only=True)\n",
    "\n",
    "# # Identify features with high skewness (threshold > 0.5 or < -0.5)\n",
    "# high_skew = skewness[abs(skewness) > 0.5]\n",
    "# print(high_skew)\n",
    "\n",
    "# # # Transforming values\n",
    "# df_frank[high_skew.index] = np.log1p(df_frank[high_skew.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{path}\\dataframes\\df_engineered.csv\", index_label=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "8sjm8lBaLOem",
    "0opgBH9ILOes",
    "3ORWG1r9kZm8",
    "ycQUvkHFkZm8",
    "FrqQV47tkZm8",
    "aY8xEIQjkZm9",
    "3qzAVpPhkZm-",
    "z1lUgXfokZnD"
   ],
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
